{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cnmnzhang/ml-concepts/blob/main/RAG_for_summarizing_research_trends_and_generating_research_ideas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG for summarizing research trends and generating research ideas"
      ],
      "metadata": {
        "id": "KnAaAQnI706l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG, vector stores, Mistral, ArXiv API\n",
        "\n",
        "Use case: Build a RAG system that summarizes trends in a **field of interest** from original, existing papers in that field. The system will also suggest research ideas with titles. We will further propmt the LLM to include a catchy title as well to see how creative the LLM can be :D\n"
      ],
      "metadata": {
        "id": "J8pHwULv7xs0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLiDBaSrzWu8"
      },
      "source": [
        "\n",
        "\n",
        "## Using RAG to enrich LLM Capabilities and address hallucination\n",
        "\n",
        "While large language models (LLMs) show powerful capabilities that power advanced use cases, they suffer from issues such as factual inconsistency and hallucination. Retrieval-augmented generation (RAG) is an approach to enrich LLM capabilities and improve reliability. RAG combines LLMs with external knowledge to enrich the prompt context with relevant information that helps accomplish a task."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Services\n",
        "Firework for fast access to open-source models\n",
        "\n",
        "Obtain your Fireworks API Key to use the Mistral 7B model: https://readme.fireworks.ai/docs\n",
        "\n",
        "Other open-source models here: https://app.fireworks.ai/models\n",
        "\n",
        "Read more about the Fireworks APIs here: https://readme.fireworks.ai/reference/createchatcompletion"
      ],
      "metadata": {
        "id": "7VgmM4Rd9U82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_query = \"genomics\" #@param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "PmGk_W_S6QAh"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsChkJxn2CSZ"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "9gy2ijb5zWu-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install chromadb tqdm fireworks-ai python-dotenv pandas\n",
        "!pip install sentence-transformers\n",
        "!pip install xmltodict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "pBSEMYFszWu_"
      },
      "outputs": [],
      "source": [
        "import fireworks.client\n",
        "import os\n",
        "import dotenv\n",
        "import chromadb\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import random\n",
        "from google.colab import userdata\n",
        "\n",
        "# you can set envs using Colab secrets\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "fireworks.client.api_key = userdata.get(\"FIREWORKS_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9v_0IEDtgov"
      },
      "source": [
        "## Getting Started with Completions from Fireworks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8wYyXMizWu_"
      },
      "source": [
        "Let's define a function to get completions from the Fireworks inference platform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1hZldHjmzWvA"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=None, max_tokens=50):\n",
        "\n",
        "    fw_model_dir = \"accounts/fireworks/models/\"\n",
        "\n",
        "    if model is None:\n",
        "        model = fw_model_dir + \"llama-v2-7b\"\n",
        "    else:\n",
        "        model = fw_model_dir + model\n",
        "\n",
        "    completion = fireworks.client.Completion.create(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    return completion.choices[0].text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys59WgrGzWvA"
      },
      "source": [
        "Let's first try the function with a simple prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "edQeSLODzWvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1f9fdcd4-04b4-442b-c06e-3170cc0dc8f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Katie and I am a 20 year old student at the University of Leeds. I am currently studying a BA in English Literature and Creative Writing. I have been working as a tutor for over 3 years now and I'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "get_completion(\"Hello, my name is\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwDjmi8EzWvB"
      },
      "source": [
        "Now let's test with Mistral-7B-Instruct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "O9TwL-2DzWvB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "39c79895-8f2e-450a-d11c-a30980fc47ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' [Your Name]. I am a [Your Profession/Occupation]. I am writing to [Purpose of Writing].\\n\\nI am writing to [Purpose of Writing] because [Reason for Writing]. I believe that ['"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "mistral_llm = \"mistral-7b-instruct-4k\"\n",
        "\n",
        "get_completion(\"Hello, my name is\", model=mistral_llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZdlBtx-zWvB"
      },
      "source": [
        "The Mistral 7B Instruct model needs to be instructed using special instruction tokens `[INST] <instruction> [/INST]` to get the right behavior. You can find more instructions on how to prompt Mistral 7B Instruct here: https://docs.mistral.ai/llm/mistral-instruct-v0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ITURzGa9zWvC",
        "outputId": "158c7a93-906f-4cf2-db03-13f1af633d0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\".\\n1. Why don't scientists trust atoms? Because they make up everything!\\n2. Did you hear about the mathematician whoâ€™s afraid of negative numbers? He will stop at nothing to avoid them.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "mistral_llm = \"mistral-7b-instruct-4k\"\n",
        "\n",
        "get_completion(\"Tell me 2 jokes\", model=mistral_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "QN6Y2y1GzWvC",
        "outputId": "db5391ea-5812-42a9-d633-adf592861cef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Sure, here are two jokes for you:\\n\\n1. Why don't scientists trust atoms? Because they make up everything!\\n2. Why did the tomato turn red? Because it saw the salad dressing!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "mistral_llm = \"mistral-7b-instruct-4k\"\n",
        "\n",
        "get_completion(\"[INST]Tell me 2 jokes[/INST]\", model=mistral_llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZHyn0tJzWvC"
      },
      "source": [
        "Now let's try with a more complex prompt that involves instructions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "dVwL--2kzWvC",
        "outputId": "fa5155c6-c74c-4b6a-ec80-631cc96d8ed9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Dear John Doe,\\n\\nWe, Tom and Mary, would like to extend our heartfelt gratitude for your attendance at our wedding. It was a pleasure to have you there, and we truly appreciate the effort you made to be a part of our special day.\\n\\nWe were thrilled to learn about your fun fact - climbing Mount Everest is an incredible accomplishment! We hope you had a safe and memorable journey.\\n\\nThank you again for joining us on this special occasion. We hope to stay in touch and catch up on all the amazing things you've been up to.\\n\\nWith love,\\n\\nTom and Mary\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "prompt = \"\"\"[INST]\n",
        "Given the following wedding guest data, write a very short 3-sentences thank you letter:\n",
        "\n",
        "{\n",
        "  \"name\": \"John Doe\",\n",
        "  \"relationship\": \"Bride's cousin\",\n",
        "  \"hometown\": \"New York, NY\",\n",
        "  \"fun_fact\": \"Climbed Mount Everest in 2020\",\n",
        "  \"attending_with\": \"Sophia Smith\",\n",
        "  \"bride_groom_name\": \"Tom and Mary\"\n",
        "}\n",
        "\n",
        "Use only the data provided in the JSON object above.\n",
        "\n",
        "The senders of the letter is the bride and groom, Tom and Mary.\n",
        "[/INST]\"\"\"\n",
        "\n",
        "get_completion(prompt, model=mistral_llm, max_tokens=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SROezW6zWvD"
      },
      "source": [
        "## RAG Use Case: ArXiv\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sw7Uk6qzWvD"
      },
      "source": [
        "### Step 1: Load the Dataset\n",
        "\n",
        "Let's first load the dataset. We will use the ArXiv API"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib, urllib.request\n",
        "\n",
        "url = f'http://export.arxiv.org/api/query?search_query=all:{search_query}&start=0&max_results=100'\n",
        "data = urllib.request.urlopen(url)\n",
        "data_decoded = data.read().decode('utf-8')\n"
      ],
      "metadata": {
        "id": "QKB3sDLSvP54"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xmltodict\n",
        "\n",
        "xml_dict = xmltodict.parse(data_decoded)\n",
        "feed = xml_dict['feed']\n",
        "ml_papers_dict = feed['entry']\n",
        "len(ml_papers_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCPdO649wrQ2",
        "outputId": "70991fbc-30a9-4825-ba77-6339c1d06857"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# papers = pd.DataFrame(entries)[['id', 'published', 'title', 'summary']]\n",
        "# papers = papers.dropna(subset=[\"title\", \"summary\"])\n",
        "# df.head(2)"
      ],
      "metadata": {
        "id": "A4hlhYQlvqw6"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, there is a [dataset](https://github.com/dair-ai/ML-Papers-of-the-Week/tree/main/research) that contains a list of weekly top trending ML papers."
      ],
      "metadata": {
        "id": "XkbIu7Pr6Hrs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "mv1z1LLczWvD"
      },
      "outputs": [],
      "source": [
        "# # load dataset from data/ folder to pandas dataframe\n",
        "# # url = 'https://raw.githubusercontent.com/dair-ai/ML-Papers-of-the-Week/main/research/ml-potw-10232023.csv'\n",
        "# ml_papers = pd.read_csv(url, index_col=0, header=0).reset_index()\n",
        "# ml_papers = ml_papers.dropna(subset=[\"Title\", \"Description\"])\n",
        "\n",
        "# # convert dataframe to list of dicts with Title and Description columns only\n",
        "# ml_papers_dict = ml_papers.to_dict(orient=\"records\")\n",
        "# len(ml_papers_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3LUGNHIzWvE",
        "outputId": "43620a30-d15e-4a7e-c36e-f8255d2a75cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'http://arxiv.org/abs/2211.08157v2',\n",
              " 'updated': '2023-01-18T17:32:16Z',\n",
              " 'published': '2022-11-15T14:09:39Z',\n",
              " 'title': 'Genome-on-Diet: Taming Large-Scale Genomic Analyses via Sparsified\\n  Genomics',\n",
              " 'summary': 'Searching for similar genomic sequences is an essential and fundamental step\\nin biomedical research and an overwhelming majority of genomic analyses.\\nState-of-the-art computational methods performing such comparisons fail to cope\\nwith the exponential growth of genomic sequencing data. We introduce the\\nconcept of sparsified genomics where we systematically exclude a large number\\nof bases from genomic sequences and enable much faster and more\\nmemory-efficient processing of the sparsified, shorter genomic sequences, while\\nproviding similar or even higher accuracy compared to processing non-sparsified\\nsequences. Sparsified genomics provides significant benefits to many genomic\\nanalyses and has broad applicability. We show that sparsifying genomic\\nsequences greatly accelerates the state-of-the-art read mapper (minimap2) by\\n2.57-5.38x, 1.13-2.78x, and 3.52-6.28x using real Illumina, HiFi, and ONT\\nreads, respectively, while providing up to 2.1x smaller memory footprint, 2x\\nsmaller index size, and more truly detected small and structural variations\\ncompared to minimap2. Sparsifying genomic sequences makes containment search\\nthrough very large genomes and very large databases 72.7-75.88x faster and\\n723.3x more storage-efficient than searching through non-sparsified genomic\\nsequences (with CMash and KMC3). Sparsifying genomic sequences enables robust\\nmicrobiome discovery by providing 54.15-61.88x faster and 720x more\\nstorage-efficient taxonomic profiling of metagenomic samples over the\\nstate-of-art tool (Metalign). We design and open-source a framework called\\nGenome-on-Diet as an example tool for sparsified genomics, which can be freely\\ndownloaded from https://github.com/CMU-SAFARI/Genome-on-Diet.',\n",
              " 'author': [{'name': 'Mohammed Alser'},\n",
              "  {'name': 'Julien Eudine'},\n",
              "  {'name': 'Onur Mutlu'}],\n",
              " 'link': [{'@href': 'http://arxiv.org/abs/2211.08157v2',\n",
              "   '@rel': 'alternate',\n",
              "   '@type': 'text/html'},\n",
              "  {'@title': 'pdf',\n",
              "   '@href': 'http://arxiv.org/pdf/2211.08157v2',\n",
              "   '@rel': 'related',\n",
              "   '@type': 'application/pdf'}],\n",
              " 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
              "  '@term': 'cs.DS',\n",
              "  '@scheme': 'http://arxiv.org/schemas/atom'},\n",
              " 'category': [{'@term': 'cs.DS', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
              "  {'@term': 'q-bio.GN', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
              "  {'@term': 'q-bio.QM', '@scheme': 'http://arxiv.org/schemas/atom'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ],
      "source": [
        "ml_papers_dict[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwfW0XoxzWvE"
      },
      "source": [
        "### Step 2: Generate Embeddings with SentenceTransformer and Store in ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "1zFDOicHzWvE"
      },
      "outputs": [],
      "source": [
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "class MyEmbeddingFunction(EmbeddingFunction):\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        batch_embeddings = embedding_model.encode(input)\n",
        "        return batch_embeddings.tolist()\n",
        "\n",
        "embed_fn = MyEmbeddingFunction()\n",
        "settings = chromadb.Settings(allow_reset=True)\n",
        "# Initialize the chromadb directory, and client.\n",
        "client = chromadb.PersistentClient(path=\"./chromadb\")\n",
        "# client.delete_collection(name=f\"arxiv_{search_query}\")\n",
        "\n",
        "# create collection\n",
        "collection = client.get_or_create_collection(\n",
        "    name=f\"arxiv_{search_query}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1c65e77291f94d0cbef990f3d840ec7d",
            "69157bf470c3467fb4c968e94b870da2",
            "5acd338605654ee89aa6688258cf3603",
            "1c76b350b1e348db909fbd31a4102a45",
            "79fedbe0e405436da070e6351cb12772",
            "c3b7dd2491474586b16bf7673b74e9c4",
            "e7ac3af60510491a9b1ba83f59c5ce68",
            "90c4b9b98e1d4df4a01778b584ca6152",
            "0aa1916b312640289731809750b3e7ac",
            "a2fc7218bef54243a63294c27d12e291",
            "d2d0049282374063ba8f48e973679684"
          ]
        },
        "id": "kUauose2zWvE",
        "outputId": "8abd98fa-42e4-4fe5-8b22-df9646de0cf5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c65e77291f94d0cbef990f3d840ec7d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Generate embeddings, and index titles in batches\n",
        "batch_size = 50\n",
        "\n",
        "# loop through batches and generated + store embeddings\n",
        "for i in tqdm(range(0, len(ml_papers_dict), batch_size)):\n",
        "\n",
        "    i_end = min(i + batch_size, len(ml_papers_dict))\n",
        "    batch = ml_papers_dict[i : i + batch_size]\n",
        "\n",
        "    # Replace title with \"No Title\" if empty string\n",
        "    batch_titles = [str(paper[\"summary\"]) if str(paper[\"summary\"]) != \"\" else \"No summary\" for paper in batch]\n",
        "    batch_ids = [str(sum(ord(c) + random.randint(1, 10000) for c in paper[\"title\"])) for paper in batch]\n",
        "    batch_metadata = [dict(url=paper[\"id\"],\n",
        "                           title=paper['title'])\n",
        "                           for paper in batch]\n",
        "\n",
        "    # generate embeddings\n",
        "    batch_embeddings = embedding_model.encode(batch_titles)\n",
        "\n",
        "    # upsert to chromadb\n",
        "    collection.upsert(\n",
        "        ids=batch_ids,\n",
        "        metadatas=batch_metadata,\n",
        "        documents=batch_titles,\n",
        "        embeddings=batch_embeddings.tolist(),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xrbURsMzWvF"
      },
      "source": [
        "### Test Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoDlxtZhzWvF",
        "outputId": "9de2838d-ac68-476f-bd78-d890cddfdb8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Consistency Models', 'Eight Things to Know about Large Language Models', 'An Overview on Language Models: Recent Developments and Outlook', 'Mastering Diverse Domains through World Models', 'Language Models can Solve Computer Tasks', 'Language Modeling is Compression', 'Large Language and Speech Model', 'Model Compression for LLMs', 'A Survey of Large Language Models', 'Toolformer: Language Models Can Teach Themselves to Use Tools']]\n"
          ]
        }
      ],
      "source": [
        "collection = client.get_or_create_collection(\n",
        "    name=f\"ml-papers-nov-2023\",\n",
        "    embedding_function=embed_fn\n",
        ")\n",
        "\n",
        "retriever_results = collection.query(\n",
        "    query_texts=[\"models\"],\n",
        "    # n_results=2,\n",
        ")\n",
        "\n",
        "print(retriever_results[\"documents\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUHeag1XzWvF"
      },
      "source": [
        "# Prompting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# user query\n",
        "# user_query = \"S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models\"\n",
        "user_query = \"genomics\"\n",
        "# query for user query\n",
        "results = collection.query(\n",
        "    query_texts=[user_query],\n",
        "    n_results=10,\n",
        ")\n",
        "\n",
        "# concatenate titles into a single string\n",
        "short_titles = '\\n'.join(results['documents'][0])\n",
        "\n",
        "prompt_template = f'''[INST]\n",
        "\n",
        "Your task one is to list machine learning trends in {search_query} and {user_query} based on the recent innovative papers in the field listed in SHORT_TITLES.\n",
        "\n",
        "Your task two is for each trend, come up with a novel research ideas. For each idea, include a catchy title and methods summary\n",
        "PLEASE DO NOT include current SHORT_TITLES in the SUGGESTED_RESEARCH.\n",
        "\n",
        "SHORT_TITLES: {short_titles}\n",
        "\n",
        "SUGGESTED_RESEARCH:\n",
        "\n",
        "[/INST]\n",
        "'''\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\nPrompt Template:\")\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpKm15Q9qwE0",
        "outputId": "ffd32eb9-1730-4145-d290-6c1725aead36"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Prompt Template:\n",
            "[INST]\n",
            "\n",
            "Your task one is to list machine learning trends in genomics and genomics based on the recent innovative papers in the field listed in SHORT_TITLES. \n",
            "\n",
            "Your task two is for each trend, come up with a novel research ideas. For each idea, include a catchy title and methods summary\n",
            "PLEASE DO NOT include current SHORT_TITLES in the SUGGESTED_RESEARCH.\n",
            "\n",
            "SHORT_TITLES: BiomedGPT\n",
            "Large language models generate functional protein sequences across diverse families\n",
            "Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl\n",
            "SequenceMatch\n",
            "Singleâ€“amino acid changes in proteins sometimes have little effect but can often lead to problems in protein folding, activity, or stability. Only a small fraction of variants have been experimentally investigated, but there are vast amounts of biological sequence data that are suitable for use as training data for machine learning approaches. Cheng et al. developed AlphaMissense, a deep learning model that builds on the protein structure prediction tool AlphaFold2 (see the Perspective by Marsh and Teichmann). The model is trained on population frequency data and uses sequence and predicted structural context, all of which contribute to its performance. The authors evaluated the model against related methods using clinical databases not included in the training and demonstrated agreement with multiplexed assays of variant effect. Predictions for all singleâ€“amino acid substitutions in the human proteome are provided as a community resource.\n",
            "scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI\n",
            "GenBench\n",
            "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
            "Emergent autonomous scientific research capabilities of large language models\n",
            "Thought Cloning\n",
            "\n",
            "SUGGESTED_RESEARCH:\n",
            "\n",
            "[/INST]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "responses = get_completion(prompt_template, model=mistral_llm, max_tokens=2000)\n",
        "suggested_titles = ''.join([str(r) for r in responses])\n",
        "\n",
        "# Print the suggestions.\n",
        "print(\"Model Suggestions:\")\n",
        "print(suggested_titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqjBlpz284kl",
        "outputId": "18ebcfea-b999-4b41-fd5a-af8f6f9e54fe"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Suggestions:\n",
            "\n",
            "Task One: Machine Learning Trends in Genomics\n",
            "\n",
            "1. Deep Learning for Protein Structure Prediction\n",
            "Title: AlphaFold2: A Deep Learning Model for Protein Structure Prediction\n",
            "Methods Summary: The AlphaFold2 model uses deep learning to predict protein structures based on amino acid sequences. The model is trained on population frequency data and uses sequence and predicted structural context to make accurate predictions. The model can be used to predict the structures of all single-amino acid substitutions in the human proteome.\n",
            "2. Interpretable Machine Learning for Science\n",
            "Title: PySR and SymbolicRegression.jl: Interpretable Machine Learning for Science\n",
            "Methods Summary: PySR and SymbolicRegression.jl are machine learning tools that are designed to be interpretable. These tools can be used to analyze scientific data and make predictions. The tools are designed to be easy to use and understand, making them a valuable resource for scientists.\n",
            "3. Single-Cell Multi-omics Using Generative AI\n",
            "Title: scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI\n",
            "Methods Summary: scGPT is a generative AI model that is designed to analyze single-cell multi-omics data. The model is trained on large datasets and can be used to make predictions about the behavior of individual cells. The model is designed to be flexible and can be used to analyze a wide range of different types of data.\n",
            "4. Predicting Million-byte Sequences with Multiscale Transformers\n",
            "Title: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers\n",
            "Methods Summary: MEGABYTE is a deep learning model that is designed to predict million-byte sequences. The model uses multiscale transformers to make accurate predictions. The model is trained on large datasets and can be used to make predictions about the behavior of complex systems.\n",
            "5. Emergent Autonomous Scientific Research Capabilities of Large Language Models\n",
            "Title: Emergent Autonomous Scientific Research Capabilities of Large Language Models\n",
            "Methods Summary: Large language models have the ability to perform autonomous scientific research. These models can be used to analyze scientific data and make predictions. The models are designed to be flexible and can be used to analyze a wide range of different types of data.\n",
            "6. Thought Cloning\n",
            "Title: Thought Cloning: A Machine Learning Approach to Replicating Human Thought\n",
            "Methods Summary: Thought cloning is a machine learning approach that is designed to replicate human thought. The approach uses deep learning to analyze brain activity and make predictions about the behavior of individual humans. The approach is still in the early stages of development and is not yet widely available.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c65e77291f94d0cbef990f3d840ec7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69157bf470c3467fb4c968e94b870da2",
              "IPY_MODEL_5acd338605654ee89aa6688258cf3603",
              "IPY_MODEL_1c76b350b1e348db909fbd31a4102a45"
            ],
            "layout": "IPY_MODEL_79fedbe0e405436da070e6351cb12772"
          }
        },
        "69157bf470c3467fb4c968e94b870da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3b7dd2491474586b16bf7673b74e9c4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e7ac3af60510491a9b1ba83f59c5ce68",
            "value": "100%"
          }
        },
        "5acd338605654ee89aa6688258cf3603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90c4b9b98e1d4df4a01778b584ca6152",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0aa1916b312640289731809750b3e7ac",
            "value": 2
          }
        },
        "1c76b350b1e348db909fbd31a4102a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2fc7218bef54243a63294c27d12e291",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d2d0049282374063ba8f48e973679684",
            "value": "â€‡2/2â€‡[00:17&lt;00:00,â€‡â€‡8.99s/it]"
          }
        },
        "79fedbe0e405436da070e6351cb12772": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3b7dd2491474586b16bf7673b74e9c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7ac3af60510491a9b1ba83f59c5ce68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90c4b9b98e1d4df4a01778b584ca6152": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0aa1916b312640289731809750b3e7ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2fc7218bef54243a63294c27d12e291": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2d0049282374063ba8f48e973679684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}